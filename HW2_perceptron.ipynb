{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYiZq0X2oB5t"
   },
   "source": [
    "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
    "\n",
    "# **HW1a The Perceptron** (20 pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGVmKzgG2Ium",
    "outputId": "4cc2ca21-861a-4fba-a38c-83e3ec04bec8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 11244  100 11244    0     0  36467      0 --:--:-- --:--:-- --:--:-- 36865\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 95  2844   95  2715    0     0  21432      0 --:--:-- --:--:-- --:--:-- 21720\n",
      "100  2844  100  2844    0     0  22384      0 --:--:-- --:--:-- --:--:-- 22752\n"
     ]
    }
   ],
   "source": [
    "# Get the datasets\n",
    "!curl.exe --output train.dat http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
    "!curl.exe --output test.dat http://huang.eng.unt.edu/CSCE-5218/test.dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A69DxPSc8vNs",
    "outputId": "5440e602-8ecd-44cf-d48d-2e8b00cdcc52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\n",
      "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
      "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
      "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
      "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\n",
      "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t0\n",
      "0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
      "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\n"
     ]
    }
   ],
   "source": [
    "# Take a peek at the datasets\n",
    "!head train.dat\n",
    "!head test.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFXHLhnhwiBR"
   },
   "source": [
    "### Build the Perceptron Model\n",
    "\n",
    "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### A13 is label not data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "cXAsP_lw3QwJ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import re\n",
    "\n",
    " \n",
    "# Corpus reader, all columns but the last one are coordinates;\n",
    "#   the last column is the label\n",
    "def read_data(file_name):\n",
    "    f = open(file_name, 'r')\n",
    "\n",
    "    data = []\n",
    "    # Discard header line\n",
    "    f.readline()\n",
    "    for instance in f.readlines():\n",
    "        if not re.search('\\t', instance): continue\n",
    "        instance = list(map(int, instance.strip().split('\\t')))\n",
    "        # Add a dummy input so that w0 becomes the bias\n",
    "        instance = [-1] + instance\n",
    "        data += [instance]\n",
    "    return data\n",
    "\n",
    "\n",
    "def dot_product(array1, array2):\n",
    "    #TODO: Return dot product of array 1 and array 2\n",
    "    dot_product_value = sum(x * y for x, y in zip(array1, array2))\n",
    "    return dot_product_value \n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    #TODO: Return outpout of sigmoid function on x\n",
    "    sigmoid_value = 1/1+ (math.pow(math.e,-x))\n",
    "    return sigmoid_value\n",
    "\n",
    "# The output of the model, which for the perceptron is \n",
    "# the sigmoid function applied to the dot product of \n",
    "# the instance and the weights\n",
    "def output(weight, instance):\n",
    "    #TODO: return the output of the model \n",
    "    dot_product_op =  dot_product(weight, instance)\n",
    "    output_value = sigmoid(dot_product_op)\n",
    "    return output_value\n",
    "\n",
    "# Predict the label of an instance; this is the definition of the perceptron\n",
    "# you should output 1 if the output is >= 0.5 else output 0\n",
    "def predict(weights, instance):\n",
    "    #TODO: return the prediction of the model\n",
    "    out = output(weights, instance)\n",
    "    label = 1 if out >= 0.5 else 0\n",
    "    return label\n",
    "\n",
    "\n",
    "# Accuracy = percent of correct predictions\n",
    "def get_accuracy(weights, instances):\n",
    "    # You do not to write code like this, but get used to it\n",
    "    #print(weights)\n",
    "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
    "                   for instance in instances])\n",
    "    #print(correct)\n",
    "    return correct * 100 / len(instances)\n",
    "\n",
    "\n",
    "# Train a perceptron with instances and hyperparameters:\n",
    "#       lr (learning rate) \n",
    "#       epochs\n",
    "# The implementation comes from the definition of the perceptron\n",
    "#\n",
    "# Training consists on fitting the parameters which are the weights\n",
    "# that's the only thing training is responsible to fit\n",
    "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
    "#\n",
    "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
    "# We are updating weights in the opposite direction of the gradient of the error,\n",
    "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
    "def train_perceptron(instances, lr, epochs):\n",
    "\n",
    "    #TODO: name this step\n",
    "    #Creating a weights list with zeros equal to number of coordinates (subtracting one from the length as the last one is the label)\n",
    "    #Initializing the weights to zero\n",
    "    \n",
    "    weights = [0] * (len(instances[0])-1)\n",
    "\n",
    "    for x in range(epochs):\n",
    "        for instance in instances:\n",
    "            #TODO: name these steps\n",
    "            #\"in_value\": Compute input value. Calculates input to the perceptron which is dot product of weights and coordinate values for the particular instance. Initially the weights will be zero as initialized.\n",
    "            #\"output\" Compute output value. Calculates output of perceptron for particular instance using activation function which is applying sigmoid function to the dot product of weights and coordinates\n",
    "            #\"error\": Compute error. Calculates the difference between actual value (label in A13 here) and predicted value of perceptron(output here)\n",
    "            in_value = dot_product(weights, instance)\n",
    "            output = sigmoid(in_value)\n",
    "            error = instance[-1] - output\n",
    "            #TODO: name these steps\n",
    "            #Updating weights\n",
    "            #In the below snippet, the weights are being updated based on learning rate, error, actual value of instance and the predicted output value. It is done to minimise the error.\n",
    "            #Fitting parameters\n",
    "            for i in range(0, len(weights)):\n",
    "                weights[i] += lr * error * output * (1-output) * instance[i]\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adBZuMlAwiBT"
   },
   "source": [
    "## Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "50YvUza-BYQF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.8546282160630142, 0.318099655580063, 0.36435300836904155, 0.36820048603468675, 0.35639079343319496, 0.37934490553596234, 0.35745501510379474, 0.41037032801795814, 0.3396954181730829, 0.3523941476605033, 0.38326771144979277, 0.29512508697886564, 0.37546692801221077, 0.8546282160630142]\n",
      "32\n",
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "lr = 0.005\n",
    "epochs = 5\n",
    "weights = train_perceptron(instances_tr, lr, epochs)\n",
    "#print(len(weights))\n",
    "accuracy = get_accuracy(weights, instances_te)\n",
    "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBXkvaiQMohX"
   },
   "source": [
    "## Questions\n",
    "\n",
    "Answer the following questions. Include your implementation and the output for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCQ6BEk1CBlr"
   },
   "source": [
    "\n",
    "\n",
    "### Question 1\n",
    "\n",
    "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
    "```\n",
    "in_value = dot_product(weights, instance)\n",
    "output = sigmoid(in_value)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "Why don't we have the following code snippet instead?\n",
    "```\n",
    "output = predict(weights, instance)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "#### TODO Add your answer here (text only)\n",
    "Answer: In order to train the percdeptron we need the error i.e, difference between the predicted value and actual value. \"Output\" in the first code snippet gives the exact predicted value after using the activation function where as in the second code snippet we are predicting the label using the output of the activation function. In order to minimise the error and increase the accuracy we need to train the perceptron using output of activation functions like sigmoid, ReLU rather than applying step function to directly predict the output with the initialised weights. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU3c3m6YL2rK"
   },
   "source": [
    "### Question 2\n",
    "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
    "\n",
    "```\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
    "lr = [0.005, 0.01, 0.05]              # learning rate\n",
    "```\n",
    "\n",
    "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
    "of your code.The output should look like the following:\n",
    "```\n",
    "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "[and so on for all the combinations]\n",
    "```\n",
    "You will get different results with different hyperparameters.\n",
    "\n",
    "#### TODO Add your answer here (code and output in the format above) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(instances_tr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "test = pandas.DataFrame(instances_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1   2   3   4   5   6   7   8   9   10  11  12  13  14\n",
       "0    -1   1   1   0   0   0   0   0   0   1   1   0   0   1   0\n",
       "1    -1   0   0   1   1   0   1   1   0   0   0   0   0   1   0\n",
       "2    -1   0   1   0   1   1   0   1   0   1   1   1   0   1   1\n",
       "3    -1   0   0   1   0   0   1   0   1   0   1   1   1   1   0\n",
       "4    -1   0   1   0   0   0   0   0   1   1   1   1   1   1   0\n",
       "..   ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
       "395  -1   1   1   0   0   0   0   1   0   0   0   0   1   1   0\n",
       "396  -1   0   0   0   1   1   0   1   0   0   1   0   0   1   0\n",
       "397  -1   1   1   0   1   0   0   0   0   0   1   1   0   1   1\n",
       "398  -1   1   1   1   1   0   1   0   1   1   1   0   1   1   1\n",
       "399  -1   0   0   0   1   0   1   1   1   0   0   1   0   1   0\n",
       "\n",
       "[400 rows x 15 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>384 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1   2   3   4   5   6   7   8   9   10  11  12  13  14\n",
       "0    -1   1   1   0   0   0   0   0   0   1   1   0   0   1   0\n",
       "1    -1   0   0   1   1   0   1   1   0   0   0   0   0   1   0\n",
       "2    -1   0   1   0   1   1   0   1   0   1   1   1   0   1   1\n",
       "3    -1   0   0   1   0   0   1   0   1   0   1   1   1   1   0\n",
       "4    -1   0   1   0   0   0   0   0   1   1   1   1   1   1   0\n",
       "..   ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
       "395  -1   1   1   0   0   0   0   1   0   0   0   0   1   1   0\n",
       "396  -1   0   0   0   1   1   0   1   0   0   1   0   0   1   0\n",
       "397  -1   1   1   0   1   0   0   0   0   0   1   1   0   1   1\n",
       "398  -1   1   1   1   1   0   1   0   1   1   1   0   1   1   1\n",
       "399  -1   0   0   0   1   0   1   1   1   0   0   1   0   1   0\n",
       "\n",
       "[384 rows x 15 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(instances_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(instances_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "G-VKJOUu2BTp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
    "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
    "\n",
    "for lr in lr_array:\n",
    "  for tr_size in tr_percent:\n",
    "    for epochs in num_epochs:\n",
    "      size =  round(len(instances_tr)*tr_size/100)\n",
    "      #print(size)\n",
    "      pre_instances = instances_tr[0:size]\n",
    "      #print(len(pre_instances))\n",
    "      weights = train_perceptron(pre_instances, lr, epochs)\n",
    "      #print((weights))\n",
    "      accuracy = get_accuracy(weights, instances_te)\n",
    "      #print(accuracy)\n",
    "      print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFB9MtwML24O"
   },
   "source": [
    "### Question 3\n",
    "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
    "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
    "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
    "   ```\n",
    "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
    "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "```\n",
    "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
    "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
    "\n",
    "#### TODO: Add your answer here (code and text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer A supporting graph:\n",
    "import pandas as pd\n",
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
    "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
    "training_size = []\n",
    "accuracies = []\n",
    "for lr in lr_array:\n",
    "  for tr_size in tr_percent:\n",
    "    \n",
    "    for epochs in num_epochs:\n",
    "      training_size.append(tr_size)\n",
    "      size =  round(len(instances_tr)*tr_size/100)\n",
    "      #print(size)\n",
    "      pre_instances = instances_tr[0:size]\n",
    "      #print(len(pre_instances))\n",
    "      weights = train_perceptron(pre_instances, lr, epochs)\n",
    "      #print((weights))\n",
    "      accuracy = get_accuracy(weights, instances_te)\n",
    "      accuracies.append(accuracy)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting\n",
    "plt.plot(training_size, accuracies, marker='o', linestyle='-')\n",
    "plt.xlabel('Training Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training size vs. Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38rA_Kp3wiBX"
   },
   "source": [
    "Answer A: This is not always the case. Taking higher amounts of data from training dataset for the training might not always lead to best possible accuracy if the overfitting occurs. Some datasets might not be of good quality they might have lot of noise, using the subsets which represent the distribution of whole dataset might achieve better results with more computational efficiency. Cross validation techniques might help us achieve better results without using whole data. Saying so, sometimes using the whole dataset might lead to better accuracy so that the model might learn better if the distribution of the test data is represented better by the whole training dataset. From the above graph we can see that training size hasn't effected the accuracy, with the data being very sparse and simple. \n",
    "\n",
    "\n",
    "Answer B: Even though the second one uses more training data, looking at the training data we can say that the data is simple and faster learning rate can help to converge to the solution better and faster. With faster learning rate the model might have skipped the local minima and achieved better accuracy with global minima.\n",
    "\n",
    "\n",
    "Answer C: There is a possibility of achieving higher accuracy with additional hyperparameters in model configuration techniques. We can use Grid search CV, Random search CV to get the best hyper parameters which yield high accuracy. We can also use different optimisation algorithms like RMS prop, ensemble methods like bagging boosting in order to achieve higher accuracy. We may also explore different models in order to achieve it. However, we should check the complexity of the model, do feature engineering to extract meaningful features in order to improve the model accuracy.\n",
    "\n",
    "Answer D: Training for more epochs keeping all other hyper parameters constant might not always be beneficial as there is risk of over fitting in the model. The computational costs also increase if the data is huge and the performance doesn't improve with increase in epochs. At the same time, it might also reduce underfitting in some models and works well with huge datasets as they require more epoch to reach the optimal solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW2_The_Perceptron.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
